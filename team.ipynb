{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f8d0a2",
   "metadata": {},
   "source": [
    "Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01be755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load the mnist dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Reshape the data to fit the model 28x28x1\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Build Model A from scratch\n",
    "model_a = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_a.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_a.fit(x_train, y_train, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de05afe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized training data shape: (60000, 32, 32, 3)\n",
      "Resized test data shape: (10000, 32, 32, 3)\n",
      "Epoch 1/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 194ms/step - accuracy: 0.8181 - loss: 0.6209 - val_accuracy: 0.9525 - val_loss: 0.1527\n",
      "Epoch 2/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 182ms/step - accuracy: 0.9488 - loss: 0.1625 - val_accuracy: 0.9590 - val_loss: 0.1209\n",
      "Epoch 3/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 179ms/step - accuracy: 0.9597 - loss: 0.1271 - val_accuracy: 0.9678 - val_loss: 0.0948\n",
      "Epoch 4/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 197ms/step - accuracy: 0.9654 - loss: 0.1098 - val_accuracy: 0.9643 - val_loss: 0.1053\n",
      "Epoch 5/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 187ms/step - accuracy: 0.9672 - loss: 0.1031 - val_accuracy: 0.9720 - val_loss: 0.0831\n",
      "Epoch 6/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 188ms/step - accuracy: 0.9705 - loss: 0.0925 - val_accuracy: 0.9717 - val_loss: 0.0893\n",
      "Epoch 7/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 185ms/step - accuracy: 0.9711 - loss: 0.0868 - val_accuracy: 0.9700 - val_loss: 0.0930\n",
      "Epoch 8/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 186ms/step - accuracy: 0.9722 - loss: 0.0822 - val_accuracy: 0.9740 - val_loss: 0.0747\n",
      "Epoch 9/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 187ms/step - accuracy: 0.9747 - loss: 0.0781 - val_accuracy: 0.9720 - val_loss: 0.0845\n",
      "Epoch 10/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 187ms/step - accuracy: 0.9752 - loss: 0.0771 - val_accuracy: 0.9710 - val_loss: 0.0870\n",
      "313/313 - 1s - 3ms/step - accuracy: 0.9922 - loss: 0.0261\n",
      "Model A Test Accuracy: 0.9922000169754028\n",
      "313/313 - 48s - 154ms/step - accuracy: 0.9733 - loss: 0.0868\n",
      "Model B Test Accuracy: 0.9732999801635742\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data for Model B\n",
    "# Convert grayscale to rgb by repeating the channels\n",
    "x_train_rgb = np.repeat(x_train, 3, axis=-1)\n",
    "x_test_rgb = np.repeat(x_test, 3, axis=-1)\n",
    "\n",
    "# Resize images to 32x32 pixels\n",
    "x_train_resized = tf.image.resize(x_train_rgb, [32, 32])\n",
    "x_test_resized = tf.image.resize(x_test_rgb, [32, 32])\n",
    "\n",
    "# Verify the shape\n",
    "print(f\"Resized training data shape: {x_train_resized.shape}\")\n",
    "print(f\"Resized test data shape: {x_test_resized.shape}\")\n",
    "\n",
    "# Build Model B transfer learning with VGG16\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(10, activation='softmax')(x)\n",
    "model_b = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_b.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_b.fit(x_train_resized, y_train, epochs=10, validation_split=0.1)\n",
    "\n",
    "# Evaluate both models\n",
    "test_loss_a, test_acc_a = model_a.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"Model A Test Accuracy: {test_acc_a}\")\n",
    "\n",
    "test_loss_b, test_acc_b = model_b.evaluate(x_test_resized, y_test, verbose=2)\n",
    "print(f\"Model B Test Accuracy: {test_acc_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd822ccb",
   "metadata": {},
   "source": [
    "To compare and contrast the performance of Model A built from scratch and Model B using transfer learning with VGG16, I look at several key metrics and aspects of the training and evaluation processes.\n",
    "\n",
    "Key Metrics\n",
    "\n",
    "Accuracy:\n",
    "Model A: 99.25%\n",
    "Model B: 97.35%\n",
    "\n",
    "Loss:\n",
    "Model A: 0.0251\n",
    "Model B: 0.0832\n",
    "\n",
    "Training Time:\n",
    "Model A: Approximately 1 second per epoch.\n",
    "Model B: Approximately 49 seconds per epoch.\n",
    "\n",
    "Detailed Comparison\n",
    "\n",
    "Accuracy\n",
    "Model A achieved a higher accuracy 99.25% compared to Model B 97.35%. This indicates that Model A, which is specifically designed and trained for the MNIST dataset, performs better in terms of correctly classifying the handwritten digits.\n",
    "\n",
    "Loss\n",
    "Model A also had a lower loss 0.0251 compared to Model B 0.0832. A lower loss indicates that Model A's predictions are closer to the actual values, suggesting better performance.\n",
    "\n",
    "Training Time\n",
    "Model B took significantly longer to train approximately 49 seconds per epoch compared to Model A approximately 1 second per epoch. This is due to the complexity of the VGG16 model, which has a much larger number of parameters and layers compared to the CNN used in Model A.\n",
    "\n",
    "Model Complexity\n",
    "Model A: A simple Convolutional Neural Network (CNN) built from scratch, specifically tailored for the MNIST dataset. It has fewer layers and parameters, making it less computationally intensive.\n",
    "Model B: Utilizes a pre-trained VGG16 model, which is a very deep and complex network originally trained on the imagenet dataset. While this model is very powerful for more complex image recognition tasks, it is somewhat overkill for the simpler MNIST dataset.\n",
    "\n",
    "Generalization and Flexibility\n",
    "Model A: While highly effective for the MNIST dataset, it might not perform as well on different datasets without significant modifications and retraining.\n",
    "Model B: The pre-trained VGG16 model can be fine-tuned for various datasets with potentially better results due to its generalization capabilities derived from training on the diverse ImageNet dataset. However, for simpler tasks like MNIST, its complexity may not be fully utilized.\n",
    "\n",
    "Transfer Learning Benefits\n",
    "Model B demonstrates the concept of transfer learning, where a pre-trained model on a large and diverse dataset can be adapted to a specific task with relatively less training data and time. However, its performance on MNIST shows that transfer learning isn't always superior, especially for simpler tasks where a specialized model can outperform a more general one.\n",
    "\n",
    "Conclusion\n",
    "Model A is more specialized and performs better on the MNIST dataset in terms of accuracy and loss, with significantly faster training times. It is less complex and computationally efficient for this specific task.\n",
    "Model B leverages the power of transfer learning with the VGG16 model, showcasing its generalization capabilities but resulting in lower accuracy and higher computational costs for the MNIST dataset.\n",
    "For the MNIST dataset, Model A is the clear winner due to its higher accuracy, lower loss, and faster training time. However, Model B illustrates the potential of transfer learning, which can be highly beneficial for more complex tasks and datasets where training a model from scratch would be more challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e9a3c6",
   "metadata": {},
   "source": [
    "Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37a1527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the airline tweets dataset\n",
    "airline_tweets = pd.read_csv('Tweets.csv')\n",
    "\n",
    "# Select relevant columns\n",
    "tweets = airline_tweets['text'].values\n",
    "labels = airline_tweets['airline_sentiment'].values\n",
    "\n",
    "# Convert labels to binary positive: 1 negative: 0\n",
    "labels = np.where(labels == 'positive', 1, 0)\n",
    "\n",
    "# Preprocess the text data\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "sequences = tokenizer.texts_to_sequences(tweets)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "165d8536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# Load the IMDB dataset\n",
    "num_words = 5000\n",
    "max_len = 100\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n",
    "\n",
    "# Pad the sequences\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1919939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12c69873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 78ms/step - accuracy: 0.8327 - loss: 0.4459 - val_accuracy: 0.8920 - val_loss: 0.2684\n",
      "Epoch 2/5\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 81ms/step - accuracy: 0.9218 - loss: 0.2058 - val_accuracy: 0.9091 - val_loss: 0.2372\n",
      "Epoch 3/5\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 86ms/step - accuracy: 0.9458 - loss: 0.1364 - val_accuracy: 0.9112 - val_loss: 0.2465\n",
      "Epoch 4/5\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - accuracy: 0.9609 - loss: 0.1046 - val_accuracy: 0.9117 - val_loss: 0.2858\n",
      "Epoch 5/5\n",
      "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 90ms/step - accuracy: 0.9698 - loss: 0.0819 - val_accuracy: 0.9052 - val_loss: 0.3288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15d048da4d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_tweets, x_test_tweets, y_train_tweets, y_test_tweets = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and train Model C\n",
    "model_c = build_model()\n",
    "model_c.fit(x_train_tweets, y_train_tweets, epochs=5, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c30b2116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 104ms/step - accuracy: 0.7064 - loss: 0.5460 - val_accuracy: 0.8303 - val_loss: 0.3886\n",
      "Epoch 2/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 121ms/step - accuracy: 0.8530 - loss: 0.3493 - val_accuracy: 0.8322 - val_loss: 0.3819\n",
      "Epoch 3/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 142ms/step - accuracy: 0.8746 - loss: 0.3100 - val_accuracy: 0.8471 - val_loss: 0.3553\n",
      "Epoch 4/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 142ms/step - accuracy: 0.8919 - loss: 0.2694 - val_accuracy: 0.8492 - val_loss: 0.3626\n",
      "Epoch 5/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 140ms/step - accuracy: 0.9062 - loss: 0.2410 - val_accuracy: 0.8410 - val_loss: 0.3895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15d05703290>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build and train Model D\n",
    "model_d = build_model()\n",
    "model_d.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b244fd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 - 1s - 14ms/step - accuracy: 0.9088 - loss: 0.3098\n",
      "Model C Test Accuracy: 0.9088114500045776\n",
      "92/92 - 2s - 17ms/step - accuracy: 0.4737 - loss: 1.0728\n",
      "Model D Test Accuracy on Airline Tweets: 0.47370219230651855\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model C\n",
    "loss_c, accuracy_c = model_c.evaluate(x_test_tweets, y_test_tweets, verbose=2)\n",
    "print(f\"Model C Test Accuracy: {accuracy_c}\")\n",
    "\n",
    "# Evaluate Model D on the airline tweets dataset\n",
    "loss_d, accuracy_d = model_d.evaluate(x_test_tweets, y_test_tweets, verbose=2)\n",
    "print(f\"Model D Test Accuracy on Airline Tweets: {accuracy_d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008d37c1",
   "metadata": {},
   "source": [
    "The results indicate the performance of both models Model C and Model D on the test subset of the airline tweets dataset. I have analyze the results:\n",
    "\n",
    "Performance Metrics\n",
    "\n",
    "Model C (Trained on Airline Tweets Dataset)\n",
    "Test Accuracy: 90.88%\n",
    "Test Loss: 0.3098\n",
    "\n",
    "Model D (Trained on IMDB Dataset)\n",
    "Test Accuracy on Airline Tweets Dataset: 47.37%\n",
    "Test Loss on Airline Tweets Dataset: 1.0728\n",
    "\n",
    "Interpretation\n",
    "Model C Performance:\n",
    "Model C achieved a high accuracy of 90.88% on the airline tweets test set. This indicates that the model is well-suited for the specific dataset it was trained on.\n",
    "Low Loss: The low loss value of 0.3098 suggests that the model's predictions are close to the true labels, further supporting its strong performance.\n",
    "\n",
    "Model D Performance:\n",
    "Model D achieved a significantly lower accuracy of 47.37% when evaluated on the airline tweets test set. This indicates that the model trained on the IMDB dataset did not generalize well to the airline tweets dataset.\n",
    "High Loss: The high loss value of 1.0728 indicates that the model's predictions are not close to the true labels, highlighting its poor performance on this dataset.\n",
    "\n",
    "Key Takeaways\n",
    "Dataset Specificity:\n",
    "Model C: Since Model C was trained on the same type of data it was tested on airline tweets, it performs very well. This demonstrates the importance of training on a dataset that closely matches the target data.\n",
    "Model D: Training on a different dataset IMDB movie reviews resulted in poor performance when tested on the airline tweets dataset. This suggests that the model did not transfer well between the different types of text data.\n",
    "\n",
    "Transfer Learning Limitations:\n",
    "The poor performance of Model D highlights the limitations of transfer learning when the source and target datasets are significantly different in terms of content and context. Although transfer learning can be powerful, its effectiveness depends on the similarity between the datasets.\n",
    "\n",
    "Model Architecture:\n",
    "Both models used the same architecture, emphasizing that the difference in performance is due to the data they were trained on rather than the model structure itself.\n",
    "\n",
    "Conclusion\n",
    "Model C is highly effective for sentiment analysis on the airline tweets dataset, achieving a high accuracy of 90.88%.\n",
    "Model D, trained on the IMDB dataset, performed poorly on the airline tweets dataset, achieving an accuracy of only 47.37%.\n",
    "These results underline the critical role of dataset relevance in training effective sentiment analysis models. For best results, models should be trained on data that closely matches the target application domain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
